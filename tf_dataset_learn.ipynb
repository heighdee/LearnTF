{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET <TFRecordDataset shapes: (), types: tf.string>\n",
      "IMAGES Tensor(\"Reshape:0\", shape=(28, 28, 1), dtype=uint8)\n",
      "LABELS Tensor(\"one_hot:0\", shape=(10,), dtype=float32)\n",
      "DATASET_1 <MapDataset shapes: ({image_raw: (28, 28, 1)}, (10,)), types: ({image_raw: tf.uint8}, tf.float32)>\n",
      "DATASET_2 <ShuffleDataset shapes: ({image_raw: (28, 28, 1)}, (10,)), types: ({image_raw: tf.uint8}, tf.float32)>\n",
      "DATASET_3 <BatchDataset shapes: ({image_raw: (?, 28, 28, 1)}, (?, 10)), types: ({image_raw: tf.uint8}, tf.float32)>\n",
      "DATASET_4 <RepeatDataset shapes: ({image_raw: (?, 28, 28, 1)}, (?, 10)), types: ({image_raw: tf.uint8}, tf.float32)>\n",
      "FEATURES {'image_raw': <tf.Tensor 'IteratorGetNext_1:0' shape=(?, 28, 28, 1) dtype=uint8>}\n",
      "LABELS Tensor(\"IteratorGetNext_1:1\", shape=(?, 10), dtype=float32)\n",
      "SESS_RUN_LABELS \n",
      " [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Validate tf.data.TFRecordDataset() using make_one_shot_iterator()\n",
    "import tensorflow as tf\n",
    "\n",
    "num_epochs = 2\n",
    "num_class = 10\n",
    "sess = tf.Session()\n",
    "\n",
    "# Use `tf.parse_single_example()` to extract data from a `tf.Example`\n",
    "# protocol buffer, and perform any additional per-record preprocessing.\n",
    "def parser(record):\n",
    "    keys_to_features = {\n",
    "        \"image_raw\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\n",
    "        \"pixels\": tf.FixedLenFeature((), tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "        \"label\": tf.FixedLenFeature((), tf.int64,\n",
    "                                    default_value=tf.zeros([], dtype=tf.int64)),\n",
    "    }\n",
    "    parsed = tf.parse_single_example(record, keys_to_features)\n",
    "\n",
    "    # Parse the string into an array of pixels corresponding to the image\n",
    "    images = tf.decode_raw(parsed[\"image_raw\"],tf.uint8)\n",
    "    images = tf.reshape(images,[28,28,1])\n",
    "    labels = tf.cast(parsed['label'], tf.int32)\n",
    "    labels = tf.one_hot(labels,num_class)\n",
    "    pixels = tf.cast(parsed['pixels'], tf.int32)\n",
    "    print(\"IMAGES\",images)\n",
    "    print(\"LABELS\",labels)\n",
    "    \n",
    "    return {\"image_raw\": images}, labels\n",
    "\n",
    "\n",
    "filenames = [\"/Users/honglan/Desktop/train_output.tfrecords\"] \n",
    "# replace the filenames with your own path\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "print(\"DATASET\",dataset)\n",
    "\n",
    "# Use `Dataset.map()` to build a pair of a feature dictionary and a label\n",
    "# tensor for each example.\n",
    "dataset = dataset.map(parser)\n",
    "print(\"DATASET_1\",dataset)\n",
    "dataset = dataset.shuffle(buffer_size=10000)\n",
    "print(\"DATASET_2\",dataset)\n",
    "dataset = dataset.batch(32)\n",
    "print(\"DATASET_3\",dataset)\n",
    "dataset = dataset.repeat(num_epochs)\n",
    "print(\"DATASET_4\",dataset)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "# `features` is a dictionary in which each value is a batch of values for\n",
    "# that feature; `labels` is a batch of labels.\n",
    "features, labels = iterator.get_next()\n",
    "\n",
    "print(\"FEATURES\",features)\n",
    "print(\"LABELS\",labels)\n",
    "print(\"SESS_RUN_LABELS \\n\",sess.run(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGES Tensor(\"Reshape:0\", shape=(28, 28, 1), dtype=uint8)\n",
      "LABELS Tensor(\"one_hot:0\", shape=(10,), dtype=float32)\n",
      "DATASET <RepeatDataset shapes: ({image_raw: (?, 28, 28, 1)}, (?, 10)), types: ({image_raw: tf.uint8}, tf.float32)>\n",
      "ITERATOR <tensorflow.python.data.ops.iterator_ops.Iterator object at 0x110df3dd8>\n",
      "FEATURES {'image_raw': <tf.Tensor 'IteratorGetNext_2:0' shape=(?, 28, 28, 1) dtype=uint8>}\n",
      "LABELS Tensor(\"IteratorGetNext_2:1\", shape=(?, 10), dtype=float32)\n",
      "TRAIN\n",
      " [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "VAL\n",
      " [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Validate tf.data.TFRecordDataset() using make_initializable_iterator()\n",
    "# In order to switch between train and validation data\n",
    "num_epochs = 2\n",
    "num_class = 10\n",
    "\n",
    "def parser(record):\n",
    "    keys_to_features = {\n",
    "        \"image_raw\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\n",
    "        \"pixels\": tf.FixedLenFeature((), tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
    "        \"label\": tf.FixedLenFeature((), tf.int64,\n",
    "                                    default_value=tf.zeros([], dtype=tf.int64)),\n",
    "    }\n",
    "    parsed = tf.parse_single_example(record, keys_to_features)\n",
    "    \n",
    "    # Parse the string into an array of pixels corresponding to the image\n",
    "    images = tf.decode_raw(parsed[\"image_raw\"],tf.uint8)\n",
    "    images = tf.reshape(images,[28,28,1])\n",
    "    labels = tf.cast(parsed['label'], tf.int32)\n",
    "    labels = tf.one_hot(labels,10)\n",
    "    pixels = tf.cast(parsed['pixels'], tf.int32)\n",
    "    print(\"IMAGES\",images)\n",
    "    print(\"LABELS\",labels)\n",
    "    \n",
    "    return {\"image_raw\": images}, labels\n",
    "\n",
    "\n",
    "filenames = tf.placeholder(tf.string, shape=[None])\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "dataset = dataset.map(parser) # Parse the record into tensors\n",
    "# print(\"DATASET\",dataset)\n",
    "dataset = dataset.shuffle(buffer_size=10000)\n",
    "dataset = dataset.batch(32)\n",
    "dataset = dataset.repeat(num_epochs)\n",
    "print(\"DATASET\",dataset)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "features, labels = iterator.get_next()\n",
    "print(\"ITERATOR\",iterator)\n",
    "print(\"FEATURES\",features)\n",
    "print(\"LABELS\",labels)\n",
    "\n",
    "\n",
    "# Initialize `iterator` with training data.\n",
    "training_filenames = [\"/Users/honglan/Desktop/train_output.tfrecords\"] \n",
    "# replace the filenames with your own path\n",
    "sess.run(iterator.initializer,feed_dict={filenames: training_filenames})\n",
    "print(\"TRAIN\\n\",sess.run(labels))\n",
    "# print(sess.run(features))\n",
    "\n",
    "# Initialize `iterator` with validation data.\n",
    "validation_filenames = [\"/Users/honglan/Desktop/val_output.tfrecords\"] \n",
    "# replace the filenames with your own path\n",
    "sess.run(iterator.initializer, feed_dict={filenames: validation_filenames})\n",
    "print(\"VAL\\n\",sess.run(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET <SkipDataset shapes: (), types: tf.string>\n",
      "DATASET_1 <MapDataset shapes: ((28, 28, 1), (10,)), types: (tf.float32, tf.float32)>\n",
      "DATASET_2 <ShuffleDataset shapes: ((28, 28, 1), (10,)), types: (tf.float32, tf.float32)>\n",
      "DATASET_3 <BatchDataset shapes: ((?, 28, 28, 1), (?, 10)), types: (tf.float32, tf.float32)>\n",
      "DATASET_4 <RepeatDataset shapes: ((?, 28, 28, 1), (?, 10)), types: (tf.float32, tf.float32)>\n",
      "FEATURES Tensor(\"IteratorGetNext_3:0\", shape=(?, 28, 28, 1), dtype=float32)\n",
      "LABELS Tensor(\"IteratorGetNext_3:1\", shape=(?, 10), dtype=float32)\n",
      "SESS_RUN_LABELS\n",
      " [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# validate tf.data.TextLineDataset() using make_one_shot_iterator()\n",
    "\n",
    "def decode_line(line):\n",
    "    # Decode the csv_line to tensor.\n",
    "    record_defaults = [[1.0] for col in range(785)]\n",
    "    items = tf.decode_csv(line, record_defaults)\n",
    "    features = items[1:785]\n",
    "    label = items[0]\n",
    "\n",
    "    features = tf.cast(features, tf.float32)\n",
    "    features = tf.reshape(features,[28,28,1])\n",
    "    label = tf.cast(label, tf.int64)\n",
    "    label = tf.one_hot(label,num_class)\n",
    "    return features,label\n",
    "\n",
    "\n",
    "filenames = [\"/Users/honglan/Desktop/train.csv\"] \n",
    "# replace the filenames with your own path\n",
    "dataset = tf.data.TextLineDataset(filenames).skip(1)\n",
    "print(\"DATASET\",dataset)\n",
    "\n",
    "# Use `Dataset.map()` to build a pair of a feature dictionary and a label\n",
    "# tensor for each example.\n",
    "dataset = dataset.map(decode_line)\n",
    "print(\"DATASET_1\",dataset)\n",
    "dataset = dataset.shuffle(buffer_size=10000)\n",
    "print(\"DATASET_2\",dataset)\n",
    "dataset = dataset.batch(32)\n",
    "print(\"DATASET_3\",dataset)\n",
    "dataset = dataset.repeat(num_epochs)\n",
    "print(\"DATASET_4\",dataset)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "# `features` is a dictionary in which each value is a batch of values for\n",
    "# that feature; `labels` is a batch of labels.\n",
    "features, labels = iterator.get_next()\n",
    "\n",
    "print(\"FEATURES\",features)\n",
    "print(\"LABELS\",labels)\n",
    "print(\"SESS_RUN_LABELS\\n\",sess.run(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET <RepeatDataset shapes: ((?, 28, 28, 1), (?, 10)), types: (tf.float32, tf.float32)>\n",
      "ITERATOR <tensorflow.python.data.ops.iterator_ops.Iterator object at 0x1112c8f60>\n",
      "FEATURES Tensor(\"IteratorGetNext_4:0\", shape=(?, 28, 28, 1), dtype=float32)\n",
      "LABELS Tensor(\"IteratorGetNext_4:1\", shape=(?, 10), dtype=float32)\n",
      "TRAIN\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "VAL\n",
      " [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# validate tf.data.TextLineDataset() using make_initializable_iterator()\n",
    "# In order to switch between train and validation data\n",
    "\n",
    "def decode_line(line):\n",
    "    # Decode the csv_line to tensor.\n",
    "    record_defaults = [[1.0] for col in range(785)]\n",
    "    items = tf.decode_csv(line, record_defaults)\n",
    "    features = items[1:785]\n",
    "    label = items[0]\n",
    "\n",
    "    features = tf.cast(features, tf.float32)\n",
    "    features = tf.reshape(features,[28,28,1])\n",
    "    label = tf.cast(label, tf.int64)\n",
    "    label = tf.one_hot(label,num_class)\n",
    "    return features,label\n",
    "\n",
    "\n",
    "filenames = tf.placeholder(tf.string, shape=[None])\n",
    "dataset = tf.data.TextLineDataset(filenames).skip(1)\n",
    "dataset = dataset.map(decode_line) # Parse the record into tensors\n",
    "# print(\"DATASET\",dataset)\n",
    "dataset = dataset.shuffle(buffer_size=10000)\n",
    "dataset = dataset.batch(32)\n",
    "dataset = dataset.repeat()\n",
    "print(\"DATASET\",dataset)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "features, labels = iterator.get_next()\n",
    "print(\"ITERATOR\",iterator)\n",
    "print(\"FEATURES\",features)\n",
    "print(\"LABELS\",labels)\n",
    "\n",
    "\n",
    "# Initialize `iterator` with training data.\n",
    "training_filenames = [\"/Users/honglan/Desktop/train.csv\"]\n",
    "sess.run(iterator.initializer,feed_dict={filenames: training_filenames})\n",
    "print(\"TRAIN\\n\",sess.run(labels))\n",
    "# print(sess.run(features))\n",
    "\n",
    "# Initialize `iterator` with validation data.\n",
    "validation_filenames = [\"/Users/honglan/Desktop/val.csv\"]\n",
    "sess.run(iterator.initializer, feed_dict={filenames: validation_filenames})\n",
    "print(\"VAL\\n\",sess.run(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "VAL\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# validate tf.data.TextLineDataset() using Reinitializable iterator\n",
    "# In order to switch between train and validation data\n",
    "\n",
    "def decode_line(line):\n",
    "    # Decode the csv_line to tensor.\n",
    "    record_defaults = [[1.0] for col in range(785)]\n",
    "    items = tf.decode_csv(line, record_defaults)\n",
    "    features = items[1:785]\n",
    "    label = items[0]\n",
    "\n",
    "    features = tf.cast(features, tf.float32)\n",
    "    features = tf.reshape(features,[28,28,1])\n",
    "    label = tf.cast(label, tf.int64)\n",
    "    label = tf.one_hot(label,num_class)\n",
    "    return features,label\n",
    "\n",
    "\n",
    "def create_dataset(filename, batch_size=32, is_shuffle=False, n_repeats=0):\n",
    "    \"\"\"create dataset for train and validation dataset\"\"\"\n",
    "    dataset = tf.data.TextLineDataset(filename).skip(1)\n",
    "    if n_repeats > 0:\n",
    "        dataset = dataset.repeat(n_repeats)         # for train\n",
    "    # dataset = dataset.map(decode_line).map(normalize)  \n",
    "    dataset = dataset.map(decode_line)    \n",
    "    # decode and normalize\n",
    "    if is_shuffle:\n",
    "        dataset = dataset.shuffle(10000)            # shuffle\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "training_filenames = [\"/Users/honglan/Desktop/train.csv\"] \n",
    "# replace the filenames with your own path\n",
    "validation_filenames = [\"/Users/honglan/Desktop/val.csv\"] \n",
    "# replace the filenames with your own path\n",
    "\n",
    "# Create different datasets\n",
    "training_dataset = create_dataset(training_filenames, batch_size=32, \\\n",
    "                                  is_shuffle=True, n_repeats=num_epochs) # train_filename\n",
    "validation_dataset = create_dataset(validation_filenames, batch_size=32, \\\n",
    "                                  is_shuffle=True, n_repeats=num_epochs) # val_filename\n",
    "\n",
    "# A reinitializable iterator is defined by its structure. We could use the\n",
    "# `output_types` and `output_shapes` properties of either `training_dataset`\n",
    "# or `validation_dataset` here, because they are compatible.\n",
    "iterator = tf.data.Iterator.from_structure(training_dataset.output_types,\n",
    "                                           training_dataset.output_shapes)\n",
    "features, labels = iterator.get_next()\n",
    "\n",
    "training_init_op = iterator.make_initializer(training_dataset)\n",
    "validation_init_op = iterator.make_initializer(validation_dataset)\n",
    "\n",
    "# Using reinitializable iterator to alternate between training and validation.\n",
    "sess.run(training_init_op)\n",
    "print(\"TRAIN\\n\",sess.run(labels))\n",
    "# print(sess.run(features))\n",
    "\n",
    "# Reinitialize `iterator` with validation data.\n",
    "sess.run(validation_init_op)\n",
    "print(\"VAL\\n\",sess.run(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      " [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "VAL\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# validate tf.data.TextLineDataset() using two different iterator\n",
    "# In order to switch between train and validation data\n",
    "\n",
    "def decode_line(line):\n",
    "    # Decode the csv_line to tensor.\n",
    "    record_defaults = [[1.0] for col in range(785)]\n",
    "    items = tf.decode_csv(line, record_defaults)\n",
    "    features = items[1:785]\n",
    "    label = items[0]\n",
    "\n",
    "    features = tf.cast(features, tf.float32)\n",
    "    features = tf.reshape(features,[28,28])\n",
    "    label = tf.cast(label, tf.int64)\n",
    "    label = tf.one_hot(label,num_class)\n",
    "    return features,label\n",
    "\n",
    "\n",
    "def create_dataset(filename, batch_size=32, is_shuffle=False, n_repeats=0):\n",
    "    \"\"\"create dataset for train and validation dataset\"\"\"\n",
    "    dataset = tf.data.TextLineDataset(filename).skip(1)\n",
    "    if n_repeats > 0:\n",
    "        dataset = dataset.repeat(n_repeats)         # for train\n",
    "    # dataset = dataset.map(decode_line).map(normalize)  \n",
    "    dataset = dataset.map(decode_line)    \n",
    "    # decode and normalize\n",
    "    if is_shuffle:\n",
    "        dataset = dataset.shuffle(10000)            # shuffle\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "training_filenames = [\"/Users/honglan/Desktop/train.csv\"] \n",
    "# replace the filenames with your own path\n",
    "validation_filenames = [\"/Users/honglan/Desktop/val.csv\"] \n",
    "# replace the filenames with your own path\n",
    "\n",
    "# Create different datasets\n",
    "training_dataset = create_dataset(training_filenames, batch_size=32, \\\n",
    "                                  is_shuffle=True, n_repeats=num_epochs) # train_filename\n",
    "validation_dataset = create_dataset(validation_filenames, batch_size=32, \\\n",
    "                                  is_shuffle=True, n_repeats=num_epochs) # val_filename\n",
    "\n",
    "# A feedable iterator is defined by a handle placeholder and its structure. We\n",
    "# could use the `output_types` and `output_shapes` properties of either\n",
    "# `training_dataset` or `validation_dataset` here, because they have\n",
    "# identical structure.\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = tf.data.Iterator.from_string_handle(\n",
    "    handle, training_dataset.output_types, training_dataset.output_shapes)\n",
    "features, labels = iterator.get_next()\n",
    "\n",
    "# You can use feedable iterators with a variety of different kinds of iterator\n",
    "# (such as one-shot and initializable iterators).\n",
    "training_iterator = training_dataset.make_one_shot_iterator()\n",
    "validation_iterator = validation_dataset.make_initializable_iterator()\n",
    "\n",
    "# The `Iterator.string_handle()` method returns a tensor that can be evaluated\n",
    "# and used to feed the `handle` placeholder.\n",
    "training_handle = sess.run(training_iterator.string_handle())\n",
    "validation_handle = sess.run(validation_iterator.string_handle())\n",
    "\n",
    "# Using different handle to alternate between training and validation.\n",
    "print(\"TRAIN\\n\",sess.run(labels, feed_dict={handle: training_handle}))\n",
    "# print(sess.run(features))\n",
    "\n",
    "# Initialize `iterator` with validation data.\n",
    "sess.run(validation_iterator.initializer)\n",
    "print(\"VAL\\n\",sess.run(labels, feed_dict={handle: validation_handle}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
